{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20acd3a6-6a69-4886-8101-4577c9e004d4",
   "metadata": {},
   "source": [
    "### Input\n",
    "Set either INPUT_FILE or INPUT_TEXT variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d6e85-c3a5-44a3-827f-d26c0d3b66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1035d2-1cda-477e-886e-a491d6c81cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide either INPUT_FILE path or INPUT_TEXT to summarize.\n",
    "INPUT_FILE=\"\" # Insert file path here\n",
    "INPUT_TEXT=\"\"\"Insert text to summarize here.\"\"\"\n",
    "\n",
    "# Style of summarization:\n",
    "\n",
    "# Numbered List style\n",
    "STYLE=\"Return your response as numbered list which covers the main points of the text.\"\n",
    "PROMPT_TRIGGER=\"NUMBERED LIST SUMMARY\"\n",
    "\n",
    "# One sentence style\n",
    "#STYLE=\"Return your response as one sentence which covers the main points of the text.\"\n",
    "#PROMPT_TRIGGER=\"ONE SENTENCE SUMMARY\"\n",
    "\n",
    "# Concise style\n",
    "#STYLE=\"Return your response as concise summary which covers the main points of the text.\"\n",
    "#PROMPT_TRIGGER=\"CONCISE SUMMARY\"\n",
    "\n",
    "# Detailed style\n",
    "#STYLE=\"Return your response as detailed summary which covers the main points of the text and key facts and figures.\"\n",
    "#PROMPT_TRIGGER=\"DETAILED SUMMARY\"\n",
    "\n",
    "# Output language, try e.g. Polish, Spanish, etc \n",
    "OUTPUT_LANGUAGE = \"English\"\n",
    "\n",
    "# Should output verbose info from underlying models, etc.\n",
    "VERBOSE=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11678989-3b16-489a-ac14-95d124f796bf",
   "metadata": {},
   "source": [
    "### Model params & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8281f5de-6163-4b98-b409-e3b488fae58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model file\n",
    "MODEL_FILE=\"./models/mistral-7b-openorca.Q5_K_M.gguf\"\n",
    "\n",
    "MODEL_CONTEXT_WINDOW=8192\n",
    "\n",
    "# Maximal lenght of model's output, in tokens.\n",
    "MAX_ANSWER_TOKENS = 2048\n",
    "\n",
    "# Chunk params in characters (not tokens).\n",
    "CHUNK_SIZE=10000\n",
    "CHUNK_OVERLAP=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45344f2-c4b3-44e6-b4dc-9d776242580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_FILE,\n",
    "    n_ctx=MODEL_CONTEXT_WINDOW,\n",
    "    # Maximal lenght of model's output, in tokens.\n",
    "    max_tokens=MAX_ANSWER_TOKENS,\n",
    "    # Don't be creative.\n",
    "    temperature=0,\n",
    "    verbose=VERBOSE,\n",
    "\n",
    "    # Remove next two lines if NOT using macOS & M1 processor:\n",
    "    n_batch=512,\n",
    "    n_gpu_layers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e35d00-6c77-425d-9079-b3f31d8bad78",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfc5bce-0bc4-4333-a369-937b999eb7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_content():\n",
    "    \"\"\"Loads INPUT_FILE if set, otherwise returns INPUT_TEXT\"\"\"\n",
    "\n",
    "    if INPUT_FILE:\n",
    "        if INPUT_FILE.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(INPUT_FILE)\n",
    "            docs = loader.load()\n",
    "            print(f\"PDF: loaded {len(docs)} pages\")\n",
    "            return \"\\n\".join([d.page_content for d in docs])\n",
    "        \n",
    "        docs =  TextLoader(INPUT_FILE).load()\n",
    "        return docs[0].page_content\n",
    "\n",
    "    return INPUT_TEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf8376f-1540-4d53-b06c-1984337f5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "Write a summary of the following text delimited by tripple backquotes.\n",
    "{style}\n",
    "\n",
    "```{content}```\n",
    "\n",
    "{trigger} in {language}:\n",
    "\"\"\"\n",
    "\n",
    "map_prompt_template = \"\"\"\n",
    "Write a concise summary of the following:\n",
    "{text}\n",
    "\n",
    "CONCISE SUMMARY in {language}:\n",
    "\"\"\"\n",
    "\n",
    "def summarize_base(llm, content):\n",
    "    \"\"\"Summarize whole content at once. The content needs to fit into model's context window.\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        combine_prompt_template\n",
    "    ).partial(\n",
    "        style=STYLE,\n",
    "        trigger=PROMPT_TRIGGER,\n",
    "        language=OUTPUT_LANGUAGE,\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, verbose=VERBOSE)\n",
    "    output = chain.run(content)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def summarize_map_reduce(llm, content):\n",
    "    \"\"\"Summarize content potentially larger that model's context window using map-reduce approach.\"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.create_documents([content])\n",
    "    print(f\"Map-Reduce content splits ({len(split_docs)} splits): {[len(sd.page_content) for sd in split_docs]}\")\n",
    "\n",
    "    map_prompt = PromptTemplate.from_template(\n",
    "        map_prompt_template\n",
    "    ).partial(\n",
    "        language=OUTPUT_LANGUAGE,\n",
    "    )\n",
    "    \n",
    "    combine_prompt = PromptTemplate.from_template(\n",
    "        combine_prompt_template\n",
    "    ).partial(\n",
    "        style=STYLE,\n",
    "        trigger=PROMPT_TRIGGER,\n",
    "        language=OUTPUT_LANGUAGE,\n",
    "    )\n",
    "\n",
    "    chain = load_summarize_chain(\n",
    "        llm=llm,\n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=map_prompt,\n",
    "        combine_prompt=combine_prompt,\n",
    "        combine_document_variable_name=\"content\",\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "\n",
    "    output = chain.run(split_docs)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1fb72-e0e8-4265-b1c1-0ad04bc74ba5",
   "metadata": {},
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d82f32-74a3-4d9c-aa27-4a4f3f200b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "INPUT_TEXT = \"\"\n",
    "INPUT_FILE = \"text.txt\"\n",
    "style = \"D\"\n",
    "\n",
    "if (style == \"A\"):\n",
    "    # Numbered List style\n",
    "    STYLE=\"Return your response as numbered list which covers the main points of the text.\"\n",
    "    PROMPT_TRIGGER=\"NUMBERED LIST SUMMARY\"\n",
    "elif (style == \"B\"):\n",
    "    # One sentence style\n",
    "    STYLE=\"Return your response as one sentence which covers the main points of the text.\"\n",
    "    PROMPT_TRIGGER=\"ONE SENTENCE SUMMARY\"\n",
    "elif (style == \"C\"):\n",
    "    # Concise style\n",
    "    STYLE=\"Return your response as concise summary which covers the main points of the text.\"\n",
    "    PROMPT_TRIGGER=\"CONCISE SUMMARY\"\n",
    "else:\n",
    "    # Detailed style\n",
    "    STYLE=\"Return your response as detailed summary which covers the main points of the text and key facts and figures.\"\n",
    "    PROMPT_TRIGGER=\"DETAILED SUMMARY\"\n",
    "\n",
    "\n",
    "content = load_content()\n",
    "content_tokens = llm.get_num_tokens(content)\n",
    "print(f\"Content length: {len(content)} chars, {content_tokens} tokens.\")\n",
    "print(\"Content sample:\\n\" + content[:200] + \"\\n\\n\")\n",
    "\n",
    "# Keep part of context window for models output.\n",
    "base_threshold = 0.75*MODEL_CONTEXT_WINDOW\n",
    "\n",
    "if (content_tokens < base_threshold):\n",
    "    print(\"Using summarizer: base\")\n",
    "    summary = summarize_base(llm, content)\n",
    "else:\n",
    "    print(\"Using summarizer: map-reduce\")\n",
    "    summary = summarize_map_reduce(llm, content)\n",
    "\n",
    "print(f\"Content length: {len(summary)} chars, {llm.get_num_tokens(summary)} tokens.\")\n",
    "print(\"Summary:\\n\" + summary + \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c360a902-0585-4a1f-8e1a-0cc06991487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Student1   Student2   Student3   Student4 Student5\n",
      "Q1    Correct    Correct    Correct    Correct  Correct\n",
      "Q2  Incorrect  Incorrect  Incorrect  Incorrect  Correct\n",
      "Q3    Correct    Correct    Correct  Incorrect  Correct\n",
      "Q4  Incorrect    Correct  Incorrect  Incorrect  Correct\n",
      "Q5  Incorrect    Correct    Correct  Incorrect  Correct\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "import pandas as pd\n",
    "\n",
    "# List of notebook filenames\n",
    "notebook_filenames = [\n",
    "    'testnotebook/1.ipynb',\n",
    "    'testnotebook/2.ipynb',\n",
    "    'testnotebook/3.ipynb',\n",
    "    'testnotebook/4.ipynb',\n",
    "    'testnotebook/5.ipynb'\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to hold all questions and their answers\n",
    "questions = set()  # To track all unique questions\n",
    "data = {filename: {} for filename in notebook_filenames}  # Initialize a dictionary for each notebook\n",
    "\n",
    "# Loop through each notebook\n",
    "for notebook_filename in notebook_filenames:\n",
    "    with open(notebook_filename, 'r') as f:\n",
    "        nb_contents = nbformat.read(f, as_version=4)\n",
    "\n",
    "        # Variable to keep track of current question\n",
    "        current_question = None\n",
    "\n",
    "        # Iterate through the cells to extract questions and answers\n",
    "        for cell in nb_contents.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                lines = cell.source.split('\\n')\n",
    "                for line in lines:\n",
    "                    if line.startswith('Q'):  # Assuming questions are prefixed with 'Q:'\n",
    "                        current_question = line.strip()  # Get the question text\n",
    "                        questions.add(current_question)  # Add to the set of questions\n",
    "                        data[notebook_filename][current_question] = None  # Initialize answer\n",
    "                    elif current_question:\n",
    "                        answer = line.strip()  # Get the answer text\n",
    "                        data[notebook_filename][current_question] = answer  # Assign answer to the current question\n",
    "\n",
    "# Create a DataFrame with all questions as rows and notebooks as columns\n",
    "df = pd.DataFrame(index=sorted(questions), columns=notebook_filenames)\n",
    "\n",
    "# Populate the DataFrame with the answers\n",
    "for notebook_id, answers in data.items():\n",
    "    for question in answers:\n",
    "        df.at[question, notebook_id] = answers[question]\n",
    "\n",
    "colName = []\n",
    "\n",
    "# Use a for loop to append numbers from 1 to 5\n",
    "for i in range(1, 6):\n",
    "    colName.append(\"Student\" + str(i))\n",
    "\n",
    "# Renaming all columns\n",
    "df.columns = colName\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ba905b6-c218-454f-acef-c36b0b5950c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content length: 335 chars, 105 tokens.\n",
      "Content sample:\n",
      "     Student1   Student2   Student3   Student4 Student5\n",
      "Q1    Correct    Correct    Correct    Correct  Correct\n",
      "Q2  Incorrect  Incorrect  Incorrect  Incorrect  Correct\n",
      "Q3    Correct    Correct    Corr\n",
      "\n",
      "\n",
      "Using summarizer: base\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Write a summary of the following text delimited by tripple backquotes.\n",
      "The input table indicates the student's performance on a question, for example Student1 answer correctly on Q1, but make mistake on Q2. Return your response as report which covers the statistic of students and which question students commonly make mistake on\n",
      "\n",
      "```     Student1   Student2   Student3   Student4 Student5\n",
      "Q1    Correct    Correct    Correct    Correct  Correct\n",
      "Q2  Incorrect  Incorrect  Incorrect  Incorrect  Correct\n",
      "Q3    Correct    Correct    Correct  Incorrect  Correct\n",
      "Q4  Incorrect    Correct  Incorrect  Incorrect  Correct\n",
      "Q5  Incorrect    Correct    Correct  Incorrect  Correct```\n",
      "\n",
      "COMMON MISTAKE in English:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Content length: 363 chars, 132 tokens.\n",
      "Summary:\n",
      "1. Student1, Q2\n",
      "2. Student2, Q2\n",
      "3. Student3, Q4\n",
      "4. Student4, Q4\n",
      "5. Student5, Q5\n",
      "\n",
      "STATISTICS:\n",
      "Total Students: 5\n",
      "Correct Answers: 10\n",
      "Incorrect Answers: 5\n",
      "Percentage of Correct Answers: 66.67%\n",
      "Percentage of Incorrect Answers: 33.33%\n",
      "```\n",
      "\n",
      "The report should include the common mistakes made by each student and the overall statistics for correct and incorrect answers.\n",
      "\n",
      "\n",
      "CPU times: total: 12min 9s\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "INPUT_TEXT = df.to_string(index=True)\n",
    "INPUT_FILE = \"\"\n",
    "style = \"E\"\n",
    "\n",
    "if (style == \"A\"):\n",
    "    # Numbered List style\n",
    "    STYLE=\"Return your response as numbered list which covers the main points of the text.\"\n",
    "    PROMPT_TRIGGER=\"NUMBERED LIST SUMMARY\"\n",
    "elif (style == \"B\"):\n",
    "    # One sentence style\n",
    "    STYLE=\"Return your response as one sentence which covers the main points of the text.\"\n",
    "    PROMPT_TRIGGER=\"ONE SENTENCE SUMMARY\"\n",
    "elif (style == \"C\"):\n",
    "    # Concise style\n",
    "    STYLE=\"Return your response as concise summary which covers the main points of the text.\"\n",
    "    PROMPT_TRIGGER=\"CONCISE SUMMARY\"\n",
    "elif (style == \"D\"):\n",
    "    # Detailed style\n",
    "    STYLE=\"Return your response as detailed summary which covers the main points of the text and key facts and figures.\"\n",
    "    PROMPT_TRIGGER=\"DETAILED SUMMARY\"\n",
    "elif (style == \"E\"):\n",
    "    # Detailed style\n",
    "    STYLE=\"The input table indicates the student's performance on a question, for example Student1 answer correctly on Q1, but make mistake on Q2. Return your response as report which covers the statistic of students and which question students commonly make mistake on\"\n",
    "    PROMPT_TRIGGER=\"COMMON MISTAKE\"\n",
    "\n",
    "content = load_content()\n",
    "content_tokens = llm.get_num_tokens(content)\n",
    "print(f\"Content length: {len(content)} chars, {content_tokens} tokens.\")\n",
    "print(\"Content sample:\\n\" + content[:200] + \"\\n\\n\")\n",
    "\n",
    "# Keep part of context window for models output.\n",
    "base_threshold = 0.75*MODEL_CONTEXT_WINDOW\n",
    "\n",
    "if (content_tokens < base_threshold):\n",
    "    print(\"Using summarizer: base\")\n",
    "    summary = summarize_base(llm, content)\n",
    "else:\n",
    "    print(\"Using summarizer: map-reduce\")\n",
    "    summary = summarize_map_reduce(llm, content)\n",
    "\n",
    "print(f\"Content length: {len(summary)} chars, {llm.get_num_tokens(summary)} tokens.\")\n",
    "print(\"Summary:\\n\" + summary + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f19006-1ae9-4835-aeba-feb18644495f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
