{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002ab54-07cd-4252-9baf-5d390d3254ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8380dc6f-2664-4b6d-baf0-f9ceeddf19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide either INPUT_FILE path or INPUT_TEXT to summarize.\n",
    "INPUT_FILE=\"\" # Insert file path here\n",
    "INPUT_TEXT=\"\"\"Insert text to summarize here.\"\"\"\n",
    "STYLE=\"\"\n",
    "PROMPT_TRIGGER=\"\"\n",
    "# Output language, try e.g. Polish, Spanish, etc \n",
    "OUTPUT_LANGUAGE = \"English\"\n",
    "\n",
    "# Should output verbose info from underlying models, etc.\n",
    "VERBOSE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a2d8cd-045c-4b85-857c-c25564832faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model file\n",
    "MODEL_FILE=\"./models/mistral-7b-openorca.Q5_K_M.gguf\"\n",
    "\n",
    "MODEL_CONTEXT_WINDOW=8192\n",
    "\n",
    "# Maximal lenght of model's output, in tokens.\n",
    "MAX_ANSWER_TOKENS = 2048\n",
    "\n",
    "# Chunk params in characters (not tokens).\n",
    "CHUNK_SIZE=10000\n",
    "CHUNK_OVERLAP=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945af425-fc31-488b-a26c-5735a647ad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_FILE,\n",
    "    n_ctx=MODEL_CONTEXT_WINDOW,\n",
    "    # Maximal lenght of model's output, in tokens.\n",
    "    max_tokens=MAX_ANSWER_TOKENS,\n",
    "    # Don't be creative.\n",
    "    temperature=0,\n",
    "    verbose=VERBOSE,\n",
    "\n",
    "    # Remove next two lines if NOT using macOS & M1 processor:\n",
    "    n_batch=512,\n",
    "    n_gpu_layers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0569827d-8524-47bd-9928-45dfb4f5b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_content():\n",
    "    \"\"\"Loads INPUT_FILE if set, otherwise returns INPUT_TEXT\"\"\"\n",
    "\n",
    "    if INPUT_FILE:\n",
    "        if INPUT_FILE.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(INPUT_FILE)\n",
    "            docs = loader.load()\n",
    "            print(f\"PDF: loaded {len(docs)} pages\")\n",
    "            return \"\\n\".join([d.page_content for d in docs])\n",
    "        \n",
    "        docs =  TextLoader(INPUT_FILE).load()\n",
    "        return docs[0].page_content\n",
    "\n",
    "    return INPUT_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5479206b-10e7-40f5-812f-02bb1e886a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "Write a summary of the following text delimited by tripple backquotes.\n",
    "{style}\n",
    "\n",
    "```{content}```\n",
    "\n",
    "{trigger} in {language}:\n",
    "\"\"\"\n",
    "\n",
    "map_prompt_template = \"\"\"\n",
    "Write a concise summary of the following:\n",
    "{text}\n",
    "\n",
    "CONCISE SUMMARY in {language}:\n",
    "\"\"\"\n",
    "\n",
    "def summarize_base(llm, content):\n",
    "    \"\"\"Summarize whole content at once. The content needs to fit into model's context window.\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        combine_prompt_template\n",
    "    ).partial(\n",
    "        style=STYLE,\n",
    "        trigger=PROMPT_TRIGGER,\n",
    "        language=OUTPUT_LANGUAGE,\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, verbose=VERBOSE)\n",
    "    output = chain.run(content)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def summarize_map_reduce(llm, content):\n",
    "    \"\"\"Summarize content potentially larger that model's context window using map-reduce approach.\"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.create_documents([content])\n",
    "    print(f\"Map-Reduce content splits ({len(split_docs)} splits): {[len(sd.page_content) for sd in split_docs]}\")\n",
    "\n",
    "    map_prompt = PromptTemplate.from_template(\n",
    "        map_prompt_template\n",
    "    ).partial(\n",
    "        language=OUTPUT_LANGUAGE,\n",
    "    )\n",
    "    \n",
    "    combine_prompt = PromptTemplate.from_template(\n",
    "        combine_prompt_template\n",
    "    ).partial(\n",
    "        style=STYLE,\n",
    "        trigger=PROMPT_TRIGGER,\n",
    "        language=OUTPUT_LANGUAGE,\n",
    "    )\n",
    "\n",
    "    chain = load_summarize_chain(\n",
    "        llm=llm,\n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=map_prompt,\n",
    "        combine_prompt=combine_prompt,\n",
    "        combine_document_variable_name=\"content\",\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "\n",
    "    output = chain.run(split_docs)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "240e0b3c-435f-4e1a-a630-eb3f21137b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "combine_prompt_template = \"\"\"\n",
    "The following text delimited by tripple backquotes is about group of students' performance in an assignment, separated by a line of dashes (----). \n",
    "Each student's data includes:\n",
    "1. The student ID\n",
    "2. The overall score for the assignment of that student\n",
    "3. A series of questions, including scores and comments for each question.\n",
    "Score: X / Y means X marks gained in a Y marks question. A question is answered correctly if X = Y. \n",
    "Write a summary of it.\n",
    "{style}\n",
    "\n",
    "```\n",
    "{content}```\n",
    "\n",
    "{trigger} in {language}:\n",
    "\"\"\"\n",
    "\n",
    "map_prompt_template = \"\"\"\n",
    "The text is about the performance of a student on assignments in a course. Each assignment is separated by a line of dashes (----).\n",
    "Each assignment's data includes:\n",
    "1. The assignment name\n",
    "2. The overall score for the assignment\n",
    "3. A series of questions, including scores and comments for each question.\n",
    "Score: X / Y means X marks gained in a Y marks question. A question is answered correctly if X = Y. \n",
    "\n",
    "Write a summary of the following text:\n",
    "{text}\n",
    "\n",
    "{trigger} in {language}:\n",
    "\"\"\"\n",
    "\n",
    "STYLE=\"Return your response as report which covers the statistic of each question. Highlight any question that most students get wrong.\"\n",
    "PROMPT_TRIGGER=\"REPORT\"\n",
    "\n",
    "def summarize(text):\n",
    "    global INPUT_TEXT\n",
    "    INPUT_TEXT = text\n",
    "    \n",
    "    content = load_content()\n",
    "    content_tokens = llm.get_num_tokens(content)\n",
    "    print(f\"Content length: {len(content)} chars, {content_tokens} tokens.\")\n",
    "    print(\"Content sample:\\n\" + content[:200] + \"\\n\\n\")\n",
    "    \n",
    "    # Keep part of context window for models output.\n",
    "    base_threshold = 0.75*MODEL_CONTEXT_WINDOW\n",
    "    \n",
    "    if (content_tokens < base_threshold):\n",
    "        print(\"Using summarizer: base\")\n",
    "        summary = summarize_base(llm, content)\n",
    "    else:\n",
    "        print(\"Using summarizer: map-reduce\")\n",
    "        summary = summarize_map_reduce(llm, content)\n",
    "    \n",
    "    print(f\"Content length: {len(summary)} chars, {llm.get_num_tokens(summary)} tokens.\")\n",
    "    print(\"Summary:\\n\" + summary + \"\\n\\n\")\n",
    "    result = \"Summary:\\n\" + summary + \"\\n\\n\"\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
